<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>王铭泽</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Mingze Wang</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="index_Chinese.html">主页&nbsp;(中文)</a></div>
<div class="menu-item"><a href="research.html">Research</a></div>
<div class="menu-item"><a href="biography.html">Biography</a></div>
<div class="menu-item"><a href="service.html">Service</a></div>
<div class="menu-item"><a href="miscellaneous.html">Miscellaneous</a></div>
<div class="menu-item"><a href="cv.pdf">CV</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>王铭泽</h1>
</div>
<table class="imgtable"><tr><td>
<img src="mingzewang.jpeg" alt="王铭泽" width="171 px" height="200 px" />&nbsp;</td>
<td align="left"><p>王铭泽 (Mingze Wang) <br />
博士候选人</p>
<p><a href="https://english.pku.edu.cn/">北京大学</a> <a href="https://www.math.pku.edu.cn/">数学科学学院</a> <br /></p>
<p>北京大学 静园六院 210 室<br />
北京大学 20 楼 210 室<br />
中国 北京, 100084<br /></p>
<p>电子邮箱: mingzewang [at] stu [dot] pku [dot] edu [dot] cn<br /></p>
<p>[<a href="https://scholar.google.com/citations?user=CkU47X0AAAAJ&amp;hl=zh-CN&amp;oi=ao">Google Scholar</a>] &nbsp;&nbsp;&nbsp; [<a href="cv.pdf">简历</a>]<br /></p>
</td></tr></table>
<h2>关于我</h2>
<p>我是<a href="https://english.pku.edu.cn/">北京大学</a><a href="https://www.math.pku.edu.cn/">数学科学学院</a>计算数学专业的最后一年直博生 (2021-现在)。 
我非常荣幸能得到<a href="https://web.math.princeton.edu/~weinan/">鄂维南</a>院士的指导。
在此之前，我于 2021 年在<a href="https://www.zju.edu.cn/">浙江大学</a><a href="http://www.math.zju.edu.cn/mathen/">数学科学学院</a>获得了数学与应用数学学士学位 (本科前三年排名为 1/111)。</p>
<div class="infoblock">
<div class="blockcontent">
<p>如果您有兴趣与我合作，请随时给我发送电子邮件。</p>
</div></div>
<div class="infoblock">
<div class="blockcontent">
<h2>News</h2>
<ul>
<li><p>[2025.10] 我获得了 &ldquo;字节跳动奖学金&rdquo; (颁发给中国和新加坡共<font color=red>20</font>名学生); 我的导师被授予<font color=red>最佳导师奖</font>。</p>
</li>
<li><p>[2025.09] 一篇论文被 NeurIPS 2025 接收，并选为 <font color=red>Spotlight</font> (<font color=red>前 3.5%</font>)。</p>
</li>
<li><p>[2025.05] 一篇文章被 ICML 2025 接收。</p>
</li>
<li><p>[2025.01] 一篇文章被 ICLR 2025 接收，并选为 <font color=red>Spotlight</font> (<font color=red>前 5.1%</font>)。</p>
</li>
<li><p>[2024.12] 我获得了 &ldquo;<font color=red>国家自然科学基金</font>青年学生基础研究项目(博士研究生)&rdquo; 资助。</p>
</li>
<li><p>[2024.09] 我获得了 &ldquo;国家奖学金 (博士)&rdquo; (全国<font color=red>前 0.2%</font>)。</p>
</li>
<li><p>[2024.09] 三篇论文被 NeurIPS 2024 接收。</p>
</li>
<li><p>[2024.05] 一篇论文被 ICML 2024 接收；一篇论文被 ACL 2024 接收。</p>
</li>
<li><p>[2023.11] 我获得了 &ldquo;北大数学研究生奖&rdquo; (<font color=red>前 1%</font>)。</p>
</li>
<li><p>[2023.09] 一篇论文被 NeurIPS 2023 接收，并选为 <font color=red>Spotlight</font> (<font color=red>前 3.5%</font>)。</p>
</li>
<li><p>[2022.11] 我通过了博士生资格考试。</p>
</li>
<li><p>[2022.10] 我获得了 &ldquo;北京大学学术创新奖&rdquo; (<font color=red>前 1%</font>)。</p>
</li>
<li><p>[2022.09] 两篇论文被 NeurIPS 2022 接收。</p>
</li>
</ul>
</div></div>
<h2>研究兴趣</h2>
<p>我对机器学习的理论、算法和应用有着广泛的兴趣。我对非凸和凸优化也很感兴趣。</p>
<p>最近，我致力于使用理论来优雅地设计算法。</p>
<p>我最近的研究课题是</p>
<ul>
<li><p><tt>深度学习理论</tt>：<tt>表达能力</tt>、<tt>优化理论</tt>、<tt>泛化理论</tt>、<tt>隐式偏好</tt>。[1][2][3][4][5][6][8][9][10][11][12][13][14][15][16][17][18][19]</p>
</li>
<li><p><tt>Transformer</tt>和<tt>大型语言模型</tt>：理论与算法，特别在大模型预训练中。[8][10][12][13][16][17][18][19][20]</p>
</li>
<li><p><tt>非凸</tt>和<tt>凸优化</tt>：理论与算法。[2][4][6][10][11][12][13][14][15][18][19]</p>
</li>
</ul>
<p>具体来说，我在深度学习理论和算法方面的研究可以被概括为:</p>
<table class="imgtable"><tr><td>
<img src="outline.png" alt="outline" width="680 px" height="270 px" />&nbsp;</td>
<td align="left"></td></tr></table>
<p>我目前的研究获得了 <tt>国家自然科学基金青年学生基础研究项目（博士研究生）</tt>的资助. (课题：<b>大模型训练中Adam优化算法的分析和改进</b>)</p>
<h2>发表论文</h2>
<p>* 表示平等贡献，† 表示主导项目.</p>
<ul>
<li><p>[20] <a href="https://arxiv.org/pdf/2510.03222"><b>Low-probability Tokens Sustain Exploration in Reinforcement Learning with Verifiable Reward</b></a> <br />
Guanhua Huang* (黄冠华), Tingqiang Xu* (许庭强), <font color=sienna size=+0.5><b><b>Mingze Wang</b></b></font> (王铭泽), Qi Yi, Xue Gong, Siheng Li, Ruibin Xiong, Kejiao Li, Yuhao Jiang, Bo Zhou (周波). <br />
under review, arXiv preprint, 1-21. Sep 2025.</p>
</li>
<li><p>[19] <a href="http://arxiv.org/abs/"><b>Towards Revealing the Effect of Batch Size Scheduling on Pre-training</b></a> <br />
Jinbo Wang* (王锦波), Binghui Li* (李柄辉), Zhanpeng Zhou (周展鹏), <font color=sienna size=+0.5><b><b>Mingze Wang</b></b></font> (王铭泽), Yuxuan Sun, Jiaqi Zhang (张家绮), Xunliang Cai (蔡勋梁), Lei Wu (吴磊). <br />
under review. Sep 2025.</p>
</li>
<li><p>[18] <a href="http://arxiv.org/abs/2505.24275"><b>GradPower: Powering Gradients for Faster Language Model Pre-Training</b></a> <br />
<font color=sienna size=+0.5><b><b>Mingze Wang*†</b></b></font> (王铭泽), Jinbo Wang* (王锦波), Jiaqi Zhang (张家绮), Wei Wang (王玮), Peng Pei (裴鹏), Xunliang Cai (蔡勋梁), Weinan E (鄂维南), Lei Wu (吴磊). <br />
under review, arXiv preprint, 1-22. May 2025.</p>
</li>
<li><p>[17] <a href="http://arxiv.org/abs/"><b>A Mechanistic Study of Transformer Training Instability under Mixed Precision</b></a> <br />
Shengtao Guo (郭胜涛), <font color=sienna size=+0.5><b><b>Mingze Wang</b></b></font> (王铭泽), Jinbo Wang (王锦波), Lei Wu (吴磊). <br />
under review. May 2025.</p>
</li>
<li><p>[16] <a href="http://arxiv.org/abs/2505.24205"><b>On the Expressive Power of Mixture-of-Experts for Structured Complex Tasks</b></a> <br />
<font color=sienna size=+0.5><b><b>Mingze Wang†</b></b></font> (王铭泽), Weinan E (鄂维南). <br />
<tt>2025 Conference on Neural Information Processing Systems</tt> (<font color=purple size=+0.5><b><b>NeurIPS 2025</b></b></font>) (<font color=red size=+0.5><b>Spotlight, 前 3.5%</b></font>), 1-18.</p>
</li>
<li><p>[15] <a href="https://openreview.net/forum?id=KfsMlrl81a"><b>On the Learning Dynamics of Two-layer ReLU Networks with Label Noise SGD</b></a> <br />
Tongcheng Zhang (张桐铖), Zhanpeng Zhou (周展鹏), <font color=sienna size=+0.5><b><b>Mingze Wang</b></b></font> (王铭泽), Andi Han, Wei Huang (黄伟), Taiji Suzuki, Junchi Yan (严骏驰). <br />
<tt>2026 AAAI Conference on Artificial Intelligence</tt> (<font color=purple size=+0.5><b><b>AAAI 2026</b></b></font>) (<font color=red size=+0.5><b>Oral</b></font>).</p>
</li>
<li><p>[14] <a href="https://openreview.net/forum?id=lWGMbJRCtQ"><b>A Single Global Merging Suffices: Recovering Centralized Learning Performance in Decentralized Learning</b></a> <br />
Tongtian Zhu (朱同天), Tianyu Zhang (张天宇), <font color=sienna size=+0.5><b><b>Mingze Wang</b></b></font> (王铭泽), Zhanpeng Zhou (周展鹏), Can Wang. <br />
<tt>ICLR 2025 Workshop Weight Space Learning</tt> (<font color=purple size=+0.5><b><b>ICLR 2025 - WSL</b></b></font>), 1-23. <br /></p>
</li>
<li><p>[13] <a href="http://arxiv.org/abs/2502.19002"><b>The Sharpness Disparity Principle in Transformers for Accelerating Language Model Pre-Training</b></a> <br />
Jinbo Wang* (王锦波), <font color=sienna size=+0.5><b><b>Mingze Wang*†</b></b></font> (王铭泽), Zhanpeng Zhou* (周展鹏), Junchi Yan (严骏驰), Weinan E (鄂维南), Lei Wu (吴磊). <br />
<tt>2025 International Conference on Machine Learning</tt> (<font color=purple size=+0.5><b><b>ICML 2025</b></b></font>), 1-23. <br /></p>
</li>
<li><p>[12] <a href="https://arxiv.org/abs/2410.11474"><b>How Transformers Get Rich: Approximation and Dynamics Analysis</b></a> <br />
<font color=sienna size=+0.5><b><b>Mingze Wang†</b></b></font>, Ruoxi Yu, Weinan E (鄂维南), Lei Wu (吴磊). <br />
<tt>ICML 2025 Workshop on High-dimensional Learning Dynamics</tt> (<font color=purple size=+0.5><b><b>ICML 2025 - HiLD</b></b></font>), 1-47.</p>
</li>
<li><p>[11] <a href="https://arxiv.org/abs/2410.10373"><b>Sharpness-Aware Minimization Efficiently Selects Flatter Minima Late in Training</b></a> <br />
Zhanpeng Zhou (周展鹏)*, <font color=sienna size=+0.5><b><b>Mingze Wang*</b></b></font> (王铭泽), Yuchen Mao, Bingrui Li (李炳睿), Junchi Yan (严骏驰). <br />
<tt>2025 International Conference on Learning Representations</tt> (<font color=purple size=+0.5><b><b>ICLR 2025</b></b></font>) (<font color=red size=+0.5><b>Spotlight, 前 5.1%</b></font>), 1-31.</p>
</li>
<li><p>[10] <a href="https://arxiv.org/abs/2405.20763"><b>Improving Generalization and Convergence by Enhancing Implicit Regularization</b></a> <br />
<font color=sienna size=+0.5><b><b>Mingze Wang†</b></b></font> (王铭泽), Jinbo Wang (王锦波), Haotian He (何浩田), Zilin Wang (王梓麟), Guanhua Huang (黄冠华), Feiyu Xiong (熊飞宇), Zhiyu Li (李志宇), Weinan E (鄂维南), Lei Wu (吴磊) <br />
<tt>2024 Conference on Neural Information Processing Systems</tt> (<font color=purple size=+0.5><b><b>NeurIPS 2024</b></b></font>), 1-44.</p>
</li>
<li><p>[9] <a href="https://arxiv.org/abs/2402.07193"><b>Loss Symmetry and Noise Equilibrium of Stochastic Gradient Descent</b></a> <br />
Liu Ziyin (刘子寅), <font color=sienna size=+0.5><b><b>Mingze Wang</b></b></font> (王铭泽), Hongchao Li, Lei Wu (吴磊) <br />
<tt>2024 Conference on Neural Information Processing Systems</tt> (<font color=purple size=+0.5><b><b>NeurIPS 2024</b></b></font>), 1-26.</p>
</li>
<li><p>[8] <a href="https://arxiv.org/abs/2402.00522"><b>Understanding the Expressive Power and Mechanisms of Transformer for Sequence Modeling</b></a> <br />
<font color=sienna size=+0.5><b><b>Mingze Wang</b></b></font> (王铭泽), Weinan E (鄂维南) <br />
<tt>2024 Conference on Neural Information Processing Systems</tt> (<font color=purple size=+0.5><b><b>NeurIPS 2024</b></b></font>), 1-76.</p>
</li>
<li><p>[7] <a href="https://arxiv.org/abs/2406.01179"><b>Are AI-Generated Text Detectors Robust to Adversarial Perturbations?</b></a> <br />
Guanhua Huang (黄冠华), Yuchen Zhang, Zhe Li, Yongjian You, <font color=sienna size=+0.5><b><b>Mingze Wang</b></b></font> (王铭泽), Zhouwang Yang (杨周旺)<br />
<tt>2024 Annual Meeting of the Association for Computational Linguistics</tt> (<font color=purple size=+0.5><b><b>ACL 2024</b></b></font>), 1-20.</p>
</li>
<li><p>[6] <a href="https://arxiv.org/abs/2311.14387"><b>Achieving Margin Maximization Exponentially Fast via Progressive Norm Rescaling</b></a> <br />
<font color=sienna size=+0.5><b><b>Mingze Wang†</b></b></font> (王铭泽), Zeping Min (闵泽平), Lei Wu (吴磊)<br />
<tt>2024 International Conference on Machine Learning</tt> (<font color=purple size=+0.5><b><b>ICML 2024</b></b></font>), 1-38.</p>
</li>
<li><p>[5] <a href="https://arxiv.org/abs/2310.00692"><b>A Theoretical Analysis of Noise Geometry in Stochastic Gradient Descent</b></a> <br />
<font color=sienna size=+0.5><b><b>Mingze Wang</b></b></font> (王铭泽), Lei Wu (吴磊)<br />
<tt>NeurIPS 2023 Workshop on Mathematics of Modern Machine Learning</tt> (<font color=purple size=+0.5><b><b>NeurIPS 2023 - M3L</b></b></font>), 1-30. <br /></p>
</li>
<li><p>[4] <a href="https://arxiv.org/abs/2305.12467"><b>Understanding Multi-phase Optimization Dynamics and Rich Nonlinear Behaviors of ReLU Networks</b></a> <br />
<font color=sienna size=+0.5><b><b>Mingze Wang†</b></b></font> (王铭泽), Chao Ma (马超)<br />
<tt>2023 Conference on Neural Information Processing Systems</tt> (<font color=purple size=+0.5><b><b>NeurIPS 2023</b></b></font>) (<font color=red size=+0.5><b>Spotlight, 前 3.5%</b></font>), 1-94.</p>
</li>
<li><p>[3] <a href="https://arxiv.org/abs/2207.02628"><b>The alignment property of SGD noise and how it helps select flat minima: A stability analysis</b></a> <br />
Lei Wu (吴磊), <font color=sienna size=+0.5><b><b>Mingze Wang</b></b></font> (王铭泽), Weijie J. Su (苏炜杰)<br />
<tt>2022 Conference on Neural Information Processing Systems</tt> (<font color=purple size=+0.5><b><b>NeurIPS 2022</b></b></font>), 1-25.</p>
</li>
<li><p>[2] <a href="https://arxiv.org/abs/2206.02139"><b>Early Stage Convergence and Global Convergence of Training Mildly Parameterized Neural Networks</b></a> <br />
<font color=sienna size=+0.5><b><b>Mingze Wang†</b></b></font> (王铭泽), Chao Ma (马超)<br />
<tt>2022 Conference on Neural Information Processing Systems</tt> (<font color=purple size=+0.5><b><b>NeurIPS 2022</b></b></font>), 1-73.</p>
</li>
<li><p>[1] <a href="https://arxiv.org/abs/2206.03299"><b>Generalization Error Bounds for Deep Neural Networks Trained by SGD</b></a> <br />
<font color=sienna size=+0.5><b><b>Mingze Wang†</b></b></font> (王铭泽), Chao Ma (马超)<br />
arXiv preprint, 1-32, June 2022.</p>
</li>
</ul>
<h2>部分奖项及荣誉</h2>
<ul>
<li><p><tt>字节跳动奖学金</tt> (颁发给中国和新加坡共<font color=red size=+0.5><b>20</b></font>名学生); 我的导师被授予<font color=red>最佳导师奖</font>, 2025.</p>
</li>
</ul>
<ul>
<li><p><tt>国家自然科学基金青年学生基础研究项目(博士研究生)</tt> (<font color=red size=+0.5><b>30万元</b></font>), 2024.</p>
</li>
</ul>
<ul>
<li><p><tt>国家奖学金 (博士)</tt> (全国<font color=red size=+0.5><b>前 0.2%</b></font>), 教育部, 2024.</p>
</li>
</ul>
<ul>
<li><p><tt>校长奖学金</tt>, 北京大学, 2024; 2025.</p>
</li>
</ul>
<ul>
<li><p><tt>北大数学研究生奖</tt> (<font color=red size=+0.5><b>前 1%</b></font>), 北京大学, 2023.</p>
</li>
</ul>
<ul>
<li><p><tt>北京大学学术创新奖</tt> (<font color=red size=+0.5><b>前 1%</b></font>), 北京大学, 2022. </p>
</li>
</ul>
<ul>
<li><p><tt>浙江省优秀毕业生</tt> (前 5%), 浙江省, 2021.</p>
</li>
</ul>
<ul>
<li><p><tt>浙江大学一等奖学金</tt> (前 3%), 浙江大学, 2019; 2020.</p>
</li>
</ul>
<ul>
<li><p><tt>国家奖学金 (本科)</tt> (全国<font color=red size=+0.5><b>前 0.2%</b></font>), 教育部, 2019.</p>
</li>
</ul>
<h2></h2>
</td>
</tr>
</table>
</body>
</html>
