<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Research</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Mingze Wang</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="research.html" class="current">Research</a></div>
<div class="menu-item"><a href="biography.html">Biography</a></div>
<div class="menu-item"><a href="service.html">Service</a></div>
<div class="menu-item"><a href="miscellaneous.html">Miscellaneous</a></div>
<div class="menu-item"><a href="cv.pdf">CV</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Research</h1>
</div>
<h2>Research Interests</h2>
<p>I am broadly interested in theory, algorithm and application of machine learning. I am also interested in non-convex and convex optimization. Specifically, my recent research topics are</p>
<ul>
<li><p><b>Deep Learning Theory</b>: optimization, generalization, and implicit bias.</p>
<ul>
<li><p><tt>Optimization</tt>: When training neural networks, why can optimization algorithms converge to global minima? [1][4]</p>
</li>
<li><p><tt>Implicit Bias</tt>: When training neural networks, why can optimization algorithms converge to global minima with favorable generalization ability (even without any explicit regularization)? Such as flat-minima-bias [2][5] and max-margin-bias aspects [4][6].</p>
</li>
<li><p><tt>Algorithm Design</tt>: For machine learning problems, design new optimization algorithms which can converge to global minima with better generalization ability. [6]</p>
</li>
<li><p><tt>Generalization</tt>: How to measure the generalization ability of neural networks. [3]</p>
</li></ul>
</li>
<li><p><b>Transformer and Large Language Model</b>: theory and algorithm.</p>
<ul>
<li><p><tt>Expressive Power</tt>: The expressive power of Transformer (On the preparation).</p>
</li>
<li><p><tt>Algorithm Design</tt>: (On the preparation).</p>
</li></ul>
</li>
<li><p><b>Non-convex and Convex Optimization</b>: theory and algorithm.</p>
<ul>
<li><p><tt>Convex Optimization in ML</tt>. [6]</p>
</li>
<li><p><tt>Non-convex Optimization in ML</tt>. [1][4]</p>
</li></ul>
</li>
<li><p><b>CV and NLP</b>: algorithm and application.</p>
</li>
<li><p><b>AI for Compositional Optimization</b>: theory and algorithm.</p>
</li>
</ul>
<h2>Recent Publications</h2>
<ul>
<li><p>[4] <a href="https://arxiv.org/abs/2305.12467"><b>Understanding Multi-phase Optimization Dynamics and Rich Nonlinear Behaviors of ReLU Networks</b></a> <br />
<b>Mingze Wang</b>, Chao Ma<br />
<tt>Thirty-seventh Conference on Neural Information Processing Systems</tt> (<b>NeurIPS 2023</b>) (<font color=red size=+0.5><b>Spotlight, Top 3.5%</b></font>).</p>
</li>
<li><p>[5] <a href="https://arxiv.org/abs/2310.00692"><b>The Noise Geometry of Stochastic Gradient Descent: A Quantitative and Analytical Characterization</b></a> <br />
<b>Mingze Wang</b>, Lei Wu<br />
<tt>NeurIPS 2023 Workshop on Mathematics of Modern Machine Learning</tt> (<b>NeurIPS 2023 - M3L</b>). <br /></p>
</li>
<li><p>[1] <a href="https://arxiv.org/abs/2206.02139"><b>Early Stage Convergence and Global Convergence of Training Mildly Parameterized Neural Networks</b></a> <br />
<b>Mingze Wang</b>, Chao Ma<br />
<tt>Thirty-sixth Conference on Neural Information Processing Systems</tt> (<b>NeurIPS 2022</b>).</p>
</li>
<li><p>[2] <a href="https://arxiv.org/abs/2207.02628"><b>The alignment property of SGD noise and how it helps select flat minima: A stability analysis</b></a> <br />
Lei Wu, <b>Mingze Wang</b>, Weijie J. Su<br />
<tt>Thirty-sixth Conference on Neural Information Processing Systems</tt> (<b>NeurIPS 2022</b>).</p>
</li>
</ul>
<h2>Recent Preprints</h2>
<ul>
<li><p>[6] <a href="https://arxiv.org/abs/2311.14387"><b>Achieving Margin Maximization Exponentially Fast via Progressive Norm Rescaling</b></a> <br />
<b>Mingze Wang</b>, Zeping Min, Lei Wu<br />
under review, Sep 2023.</p>
</li>
<li><p>[3] <a href="https://arxiv.org/abs/2206.03299"><b>Generalization Error Bounds for Deep Neural Networks Trained by SGD</b></a> <br />
<b>Mingze Wang</b>, Chao Ma<br />
arXiv preprint, June 2022.</p>
</li>
</ul>
<h2>Co-authors</h2>
<ul>
<li><p><a href="https://www.chaom.org/"><b>Chao Ma</b>.</a> Department of Mathematics, Stanford University.</p>
</li>
</ul>
<ul>
<li><p><a href="https://leiwu0.github.io/index.html"><b>Lei Wu</b>.</a> School of Mathematical Sciences, Peking University.</p>
</li>
</ul>
<ul>
<li><p><a href="http://stat.wharton.upenn.edu/~suw/"><b>Weijie J. Su</b>.</a> Department of Statistics and Data Science, University of Pennsylvania.</p>
</li>
</ul>
</td>
</tr>
</table>
</body>
</html>
