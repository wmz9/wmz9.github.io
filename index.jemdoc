# jemdoc: menu{MENU}{index.html}, nofooter
# jemdoc: addcss{jemdoc.css}, notime
==Mingze Wang

~~~
{}{img_left}{mingzewang.jpeg}{Mingze Wang}{171 px}{200 px}
Mingze Wang (王铭泽) \n
Ph.D. Candidate

[https://www.math.pku.edu.cn/ School of Mathematical Sciences] \n
[https://english.pku.edu.cn/ Peking University]

210, Jingyuan Building \#6 (静园六院), Peking University\n
210, Building \#20 (20楼), Peking University\n
Beijing, China, 100084\n

Email: mingzewang \[at\] stu \[dot\] pku \[dot\] edu \[dot\] cn\n

\[[https://scholar.google.com/citations?user=CkU47X0AAAAJ&hl=zh-CN&oi=ao Google Scholar]\] ~~~ \[[cv.pdf CV]\]\n
~~~


== About me
I am a final-year Ph.D candidate in Computational Mathematics, [https://www.math.pku.edu.cn/ School of Mathematical Sciences], [https://english.pku.edu.cn/ Peking University] (2021-Present). I am very fortunate to be advised by [https://web.math.princeton.edu/~weinan/ Prof. Weinan E]. Prior to that, I received my B.S. degree in Pure and Applied Mathematics (ranking 1/111 for the first three years during my undergraduate study) from [http://www.math.zju.edu.cn/mathen/ School of Mathematical Sciences], [https://www.zju.edu.cn/ Zhejiang University], Hangzhou, China in 2021.

~~~
Please feel free to drop me an email if you are interested in collaborating with me.
~~~


~~~
== News
- \[2026.01\] Two paper accepted to ICLR 2026. One of them was selected for an {{<font color=red>}}Oral{{</font>}} ({{<font color=red>}}top 1.2%{{</font>}}).
- \[2025.11\] I won the 2025 ByteDance Scholarship (awarded to {{<font color=red>}}20 students{{</font>}} in China and Singapore).
- \[2025.09\] One paper accepted to NeurIPS 2025 as a {{<font color=red>}}Spotlight{{</font>}} ({{<font color=red>}}top 3.5%{{</font>}}).
- \[2025.05\] One paper accepted to ICML 2025. 
- \[2025.01\] One paper accepted to ICLR 2025 as a {{<font color=red>}}Spotlight{{</font>}} ({{<font color=red>}}top 5.1%{{</font>}}).
- \[2024.12\] I received support from the {{<font color=red>}}Young Scientists (Ph.D) Fund{{</font>}} of the National Natural Science Foundation of China.
- \[2024.09\] I won the 2024 China National Scholarship ({{<font color=red>}}top 0.2%{{</font>}} in the nation).
- \[2024.09\] Three papers accepted to NeurIPS 2024.
- \[2024.05\] One paper accepted to ICML 2024. One paper accepted to ACL 2024.
- \[2023.11\] I won the 2023 BICMR Mathematical Award for Graduate Students ({{<font color=red>}}top 1%{{</font>}}).
- \[2023.09\] One paper accepted to NeurIPS 2023 as a {{<font color=red>}}Spotlight{{</font>}} ({{<font color=red>}}top 3.5%{{</font>}}).
- \[2022.11\] I have passed the Ph.D. qualifying exam.
- \[2022.10\] I won the 2022 PKU Academic Innovation Award ({{<font color=red>}}top 1%{{</font>}}).
- \[2022.09\] Two papers accepted to NeurIPS 2022.
~~~


== Research Interests
I am broadly interested in theory, algorithm and application of machine learning. I am also interested in non-convex and convex optimization. 

Recently, I am dedicated to use theory to design algorithms elegantly. 

My recent research topics are
- *Deep Learning Theory*: *expressivity*, *optimization*, *generalization*, *implicit bias*. \[1\]\[2\]\[3\]\[4\]\[5\]\[6\]\[8\]\[9\]\[10\]\[11\]\[12\]\[13\]\[14\]\[15\]\[16\]\[17\]\[18\]\[19\]\[20\]
- *Transformer* and *Large Language Model*: theory and algorithm, especially in LLM pre-training. \[8\]\[10\]\[12\]\[13\]\[16\]\[17\]\[18\]\[19\]\[20\]
- *Non-convex* and *Convex Optimization*: theory and algorithm. \[2\]\[4\]\[6\]\[10\]\[11\]\[12\]\[13\]\[14\]\[15\]\[18\]\[19\]

Specifically, my research on deep learning theory and algorithm can be summarized as:
~~~
{}{img_left}{outline.png}{outline}{680 px}{270 px}
~~~

My current research is supported the +Young Scientists (Ph.D) Fund of the National Natural Science Foundation of China+. (*Analyzing and Improving the Adam Optimizer for Foundation Model
Training*)



== Recent Publications and Preprints

\* indicates equal contribution, † means project lead.

- \[20\] [http://arxiv.org/abs/ *Accelerating LLM Pre-training through Flat-Direction Dynamics Enhancement*] \n
Shuchen Zhu, Rizhen Hu, {{<font color=sienna size=+0.5><b>}}*Mingze Wang†*{{</b></font>}}, Mou Sun, Xue Wang, Kun Yuan, Zaiwen Wen. \n
under review, arXiv preprint, 1-43. Feb 2026.
- \[19\] [https://arxiv.org/pdf/2510.03222 *Low-probability Tokens Sustain Exploration in Reinforcement Learning with Verifiable Reward*] \n
Guanhua Huang\*, Tingqiang Xu\*, {{<font color=sienna size=+0.5><b>}}*Mingze Wang*{{</b></font>}}, Qi Yi, Xue Gong, Siheng Li, Ruibin Xiong, Kejiao Li, Yuhao Jiang, Bo Zhou. \n
under review, arXiv preprint, 1-21. Sep 2025.
- \[18\] [https://arxiv.org/abs/2602.14208 *Towards Revealing the Effect of Batch Size Scheduling on Pre-training*] \n
Jinbo Wang\*, Binghui Li\*, Zhanpeng Zhou, {{<font color=sienna size=+0.5><b>}}*Mingze Wang*{{</b></font>}}, Yuxuan Sun, Jiaqi Zhang, Xunliang Cai, Lei Wu. \n
+2026 International Conference on Learning Representations+ ({{<font color=purple size=+0.5><b>}}*ICLR 2026*{{</b></font>}}).
- \[17\] [http://arxiv.org/abs/2505.24275 *GradPower: Powering Gradients for Faster Language Model Pre-Training*] \n
\ {{<font color=sienna size=+0.5><b>}}*Mingze Wang\*†*{{</b></font>}}, Jinbo Wang\*, Jiaqi Zhang, Wei Wang, Peng Pei, Xunliang Cai, Weinan E, Lei Wu. \n
under review, arXiv preprint, 1-22. May 2025.
- \[16\] [http://arxiv.org/abs/2505.24205 *On the Expressive Power of Mixture-of-Experts for Structured Complex Tasks*] \n
\ {{<font color=sienna size=+0.5><b>}}*Mingze Wang†*{{</b></font>}}, Weinan E. \n
+2025 Conference on Neural Information Processing Systems+ ({{<font color=purple size=+0.5><b>}}*NeurIPS 2025*{{</b></font>}}) ({{<font color=red size=+0.5><b>}}Spotlight, top 3.5%{{</b></font>}}), 1-18.
- \[15\] [https://openreview.net/forum?id=KfsMlrl81a *On the Learning Dynamics of Two-layer Linear Networks with Label Noise SGD*] \n
Tongcheng Zhang, Zhanpeng Zhou, {{<font color=sienna size=+0.5><b>}}*Mingze Wang*{{</b></font>}}, Andi Han, Wei Huang, Taiji Suzuki, Junchi Yan. \n
+2026 AAAI Conference on Artificial Intelligence+ ({{<font color=purple size=+0.5><b>}}*AAAI 2026*{{</b></font>}}) ({{<font color=red size=+0.5><b>}}Oral{{</b></font>}}).
- \[14\] [https://openreview.net/forum?id=lWGMbJRCtQ *A Single Global Merging Suffices: Recovering Centralized Learning Performance in Decentralized Learning*] \n
Tongtian Zhu, Tianyu Zhang, {{<font color=sienna size=+0.5><b>}}*Mingze Wang*{{</b></font>}}, Zhanpeng Zhou, Can Wang. \n
+2026 International Conference on Learning Representations+ ({{<font color=purple size=+0.5><b>}}*ICLR 2026*{{</b></font>}}) ({{<font color=red size=+0.5><b>}}Oral, top 1.2%{{</b></font>}}), 1-23.
- \[13\] [http://arxiv.org/abs/2502.19002 *The Sharpness Disparity Principle in Transformers for Accelerating Language Model Pre-Training*] \n
Jinbo Wang\*, {{<font color=sienna size=+0.5><b>}}*Mingze Wang\*†*{{</b></font>}}, Zhanpeng Zhou\*, Junchi Yan, Weinan E, Lei Wu. \n
+2025 International Conference on Machine Learning+ ({{<font color=purple size=+0.5><b>}}*ICML 2025*{{</b></font>}}), 1-23. \n
- \[12\] [https://arxiv.org/abs/2410.11474 *How Transformers Get Rich: Approximation and Dynamics Analysis*] \n
\ {{<font color=sienna size=+0.5><b>}}*Mingze Wang†*{{</b></font>}}, Ruoxi Yu, Weinan E, Lei Wu. \n
+ICML 2025 Workshop on High-dimensional Learning Dynamics+ ({{<font color=purple size=+0.5><b>}}*ICML 2025 - HiLD*{{</b></font>}}), 1-47.
- \[11\] [https://arxiv.org/abs/2410.10373 *Sharpness-Aware Minimization Efficiently Selects Flatter Minima Late in Training*] \n
Zhanpeng Zhou\*, {{<font color=sienna size=+0.5><b>}}*Mingze Wang\**{{</b></font>}}, Yuchen Mao, Bingrui Li, Junchi Yan. \n
+2025 International Conference on Learning Representations+ ({{<font color=purple size=+0.5><b>}}*ICLR 2025*{{</b></font>}}) ({{<font color=red size=+0.5><b>}}Spotlight, top 5.1%{{</b></font>}}), 1-31.
- \[10\] [https://arxiv.org/abs/2405.20763 *Improving Generalization and Convergence by Enhancing Implicit Regularization*] \n
\ {{<font color=sienna size=+0.5><b>}}*Mingze Wang†*{{</b></font>}}, Jinbo Wang, Haotian He, Zilin Wang, Guanhua Huang, Feiyu Xiong, Zhiyu Li, Weinan E, Lei Wu \n
+2024 Conference on Neural Information Processing Systems+ ({{<font color=purple size=+0.5><b>}}*NeurIPS 2024*{{</b></font>}}), 1-44.
- \[9\] [https://arxiv.org/abs/2402.07193 *Loss Symmetry and Noise Equilibrium of Stochastic Gradient Descent*] \n
Liu Ziyin, {{<font color=sienna size=+0.5><b>}}*Mingze Wang*{{</b></font>}}, Hongchao Li, Lei Wu \n
+2024 Conference on Neural Information Processing Systems+ ({{<font color=purple size=+0.5><b>}}*NeurIPS 2024*{{</b></font>}}), 1-26.
- \[8\] [https://arxiv.org/abs/2402.00522 *Understanding the Expressive Power and Mechanisms of Transformer for Sequence Modeling*] \n
\ {{<font color=sienna size=+0.5><b>}}*Mingze Wang*{{</b></font>}}, Weinan E \n
+2024 Conference on Neural Information Processing Systems+ ({{<font color=purple size=+0.5><b>}}*NeurIPS 2024*{{</b></font>}}), 1-76.
- \[7\] [https://arxiv.org/abs/2406.01179 *Are AI-Generated Text Detectors Robust to Adversarial Perturbations?*] \n
Guanhua Huang, Yuchen Zhang, Zhe Li, Yongjian You, {{<font color=sienna size=+0.5><b>}}*Mingze Wang*{{</b></font>}}, Zhouwang Yang\n
+2024 Annual Meeting of the Association for Computational Linguistics+ ({{<font color=purple size=+0.5><b>}}*ACL 2024*{{</b></font>}}), 1-20.
- \[6\] [https://arxiv.org/abs/2311.14387 *Achieving Margin Maximization Exponentially Fast via Progressive Norm Rescaling*] \n
\ {{<font color=sienna size=+0.5><b>}}*Mingze Wang†*{{</b></font>}}, Zeping Min, Lei Wu\n
+2024 International Conference on Machine Learning+ ({{<font color=purple size=+0.5><b>}}*ICML 2024*{{</b></font>}}), 1-38.
- \[5\] [https://arxiv.org/abs/2310.00692 *A Theoretical Analysis of Noise Geometry in Stochastic Gradient Descent*] \n
\ {{<font color=sienna size=+0.5><b>}}*Mingze Wang*{{</b></font>}}, Lei Wu\n
+NeurIPS 2023 Workshop on Mathematics of Modern Machine Learning+ ({{<font color=purple size=+0.5><b>}}*NeurIPS 2023 - M3L*{{</b></font>}}), 1-30.
- \[4\] [https://arxiv.org/abs/2305.12467 *Understanding Multi-phase Optimization Dynamics and Rich Nonlinear Behaviors of ReLU Networks*] \n
\ {{<font color=sienna size=+0.5><b>}}*Mingze Wang†*{{</b></font>}}, Chao Ma\n
+2023 Conference on Neural Information Processing Systems+ ({{<font color=purple size=+0.5><b>}}*NeurIPS 2023*{{</b></font>}}) ({{<font color=red size=+0.5><b>}}Spotlight, top 3.5%{{</b></font>}}), 1-94.
- \[3\] [https://arxiv.org/abs/2207.02628 *The alignment property of SGD noise and how it helps select flat minima: A stability analysis*] \n
Lei Wu, {{<font color=sienna size=+0.5><b>}}*Mingze Wang*{{</b></font>}}, Weijie J. Su\n
+2022 Conference on Neural Information Processing Systems+ ({{<font color=purple size=+0.5><b>}}*NeurIPS 2022*{{</b></font>}}), 1-25.
- \[2\] [https://arxiv.org/abs/2206.02139 *Early Stage Convergence and Global Convergence of Training Mildly Parameterized Neural Networks*] \n
\ {{<font color=sienna size=+0.5><b>}}*Mingze Wang†*{{</b></font>}}, Chao Ma\n
+2022 Conference on Neural Information Processing Systems+ ({{<font color=purple size=+0.5><b>}}*NeurIPS 2022*{{</b></font>}}), 1-73.
- \[1\] [https://arxiv.org/abs/2206.03299 *Generalization Error Bounds for Deep Neural Networks Trained by SGD*] \n
\ {{<font color=sienna size=+0.5><b>}}*Mingze Wang†*{{</b></font>}}, Chao Ma\n
arXiv preprint, 1-32, June 2022.


== Selected Awards and Honours

- +ByteDance Scholarship+ (awarded to {{<font color=red size=+0.5><b>}}20 students{{</b></font>}} in China and Singapore); my advisor received the {{<font color=red size=+0.5><b>}}best mentor award{{</b></font>}}, 2025.

- +Young Scientists (Ph.D) Fund of the National Natural Science Foundation of China+ ({{<font color=red size=+0.5><b>}}¥300,000{{</b></font>}}), 2024.

- +China National Scholarship+ ({{<font color=red size=+0.5><b>}}top 0.2%{{</b></font>}} in the nation), The Ministry of Education, 2024.

- +Principal Scholarship+, Peking University, 2024; 2025.

- +BICMR Mathematical Award for Graduate Students+ ({{<font color=red size=+0.5><b>}}top 1%{{</b></font>}}), Peking University, 2023.

- +PKU Academic Innovation Award+ ({{<font color=red size=+0.5><b>}}top 1%{{</b></font>}}), Peking University, 2022. 

- +Outstanding Graduate of Zhejiang Province+ (top 5%), Zhejiang Province, 2021.

- +First Class Scholarship of ZJU+ (top 3%), Zhejiang University, 2019; 2020.

- +China National Scholarship+ ({{<font color=red size=+0.5><b>}}top 0.2%{{</b></font>}} in the nation), The Ministry of Education, 2019.


== Selected Experience

+Foundation Models+

- [https://seed.bytedance.com/zh/ *ByteDance*], *Seed Edge* group, Beijing, China.
  -- Algorithm Intern (+Topseed intern+) (2026.1-now).
  -- Frontier research of foundation models.

- [https://www.tencent.com/zh-cn/ *Tencent*], *LLM post-training* group, Beijing, China.
  -- Algorithm Intern (+Qingyun intern+) (2025.6-2025.7).
  -- Work on designing verifiable rewards of reinforcement learning for LLM post-training.

- [https://www.meituan.com/en-US/about-us *Meituan*], *LLM pre-training* group, Beijing, China.
  -- Algorithm Intern (2025.1-2025.5).
  -- Work on designing stable and faster optimization algorithms for LLM pre-training.

- [https://www.iaar.ac.cn/ *Institute for Advanced Algorithms Research*], *LLM group*, Shanghai, China.
  -- Algorithm Intern (2023.12-2024.8).
  -- Work on designing faster optimization algorithms for LLM pre-training.

+Quantitative Trading+

- [https://zding.fund/ *Definite Capital Management*], Beijing, China.
  -- Algorithm Intern (2025.10-2025.12).
  -- Work on designing better deep learning models and training strategies in quantitative trading.

- [https://www.wizardquant.com/ *Wizard Quant*], Beijing, China.
  -- Algorithm Intern (2025.8.1).
  -- Work on designing better machine learning models in quantitative trading.


== 
