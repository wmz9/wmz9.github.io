<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Research</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Mingze Wang</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="index_Chinese.html">主页&nbsp;(中文)</a></div>
<div class="menu-item"><a href="research.html" class="current">Research</a></div>
<div class="menu-item"><a href="biography.html">Biography</a></div>
<div class="menu-item"><a href="service.html">Service</a></div>
<div class="menu-item"><a href="miscellaneous.html">Miscellaneous</a></div>
<div class="menu-item"><a href="cv.pdf">CV</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Research</h1>
</div>
<h2>Research Interests</h2>
<p>I am broadly interested in theory, algorithm and application of machine learning. I am also interested in non-convex and convex optimization. </p>
<p>Recently, I am dedicated to to use theory to design algorithms elegantly. </p>
<p>Specifically, my recent research topics are</p>
<ul>
<li><p><b>Deep Learning Theory</b>: theory and theory-inspired algorithms.[1][2][3][4][5][6][8][9][10][11][12][13][14][17][18][19]</p>
<ul>
<li><p><tt>Expressivity</tt>: Exploring the expressive power of Transformers through the lens of approximation theory [8][12]; the expressivity of the expressivity of mixture-of-experts models (MoE) [16].</p>
</li>
<li><p><tt>Optimization</tt>: When training neural networks, why can optimization algorithms converge to global minima? [2][4][12]</p>
</li>
<li><p><tt>Implicit Bias</tt>: When training neural networks, why can optimization algorithms converge to global minima with favorable generalization ability (even without any explicit regularization)? Such as flat-minima-bias [3][5][9][10][11] and max-margin-bias aspects [4][6].</p>
</li>
<li><p><tt>Generalization</tt>: How to measure the generalization ability of neural networks. [1]</p>
</li>
<li><p><tt>Algorithm Design</tt>: For machine learning problems, design new provable optimization algorithms which can which can (i) converge faster [10][13][17][18][19]; (ii) generalize better [6][10].</p>
</li></ul>
</li>
<li><p><b>Transformer and Large Language Model</b>: theory and algorithm, especially in LLM pre-training. [8][10][12][13][17][18][19][20]</p>
<ul>
<li><p><tt>Expressive Power</tt>: The expressive power and mechanisms of Transformer [8][12]; the expressivity of mixture-of-experts models (MoE)[16]; the mechanisms of in-context learning[12].</p>
</li>
<li><p><tt>Algorithm Design</tt>: Design provable faster optimizers for training LLMs [10][13][17][18][19][20]; design more efficient model architectures.</p>
</li></ul>
</li>
<li><p><b>Non-convex and Convex Optimization</b>: theory and algorithm. [2][4][6][10][11][12][13][14][17][18][19]</p>
<ul>
<li><p><tt>Convex Optimization in ML</tt>. [6]</p>
</li>
<li><p><tt>Non-convex Optimization in ML</tt>. [2][4][10][11][12][13][14][17][18][19]</p>
</li>
<li><p><tt>Algorithm Design</tt>: Design provable faster / more stable optimizers for training neural networks [10][13][17][18]; accelerate the convergence for the problems with specific structure [6].</p>
</li>
</ul>

</li>
</ul>
<h2>Recent Publications and Preprints</h2>
<p>* indicates equal contribution, † means project lead.</p>
<ul>
<li><p>[20] <a href="https://arxiv.org/pdf/2510.03222"><b>Low-probability Tokens Sustain Exploration in Reinforcement Learning with Verifiable Reward</b></a> <br />
Guanhua Huang*, Tingqiang Xu*, <font color=sienna size=+0.5><b><b>Mingze Wang</b></b></font>, Qi Yi, Xue Gong, Siheng Li, Ruibin Xiong, Kejiao Li, Yuhao Jiang, Bo Zhou. <br />
under review, arXiv preprint, 1-21. Sep 2025.</p>
</li>
<li><p>[19] <a href="http://arxiv.org/abs/"><b>Towards Revealing the Effect of Batch Size Scheduling on Pre-training</b></a> <br />
Jinbo Wang*, Binghui Li*, Zhanpeng Zhou, <font color=sienna size=+0.5><b><b>Mingze Wang</b></b></font>, Yuxuan Sun, Jiaqi Zhang, Xunliang Cai, Lei Wu. <br />
under review. Sep 2025.</p>
</li>
<li><p>[18] <a href="http://arxiv.org/abs/2505.24275"><b>GradPower: Powering Gradients for Faster Language Model Pre-Training</b></a> <br />
<font color=sienna size=+0.5><b><b>Mingze Wang*†</b></b></font>, Jinbo Wang*, Jiaqi Zhang, Wei Wang, Peng Pei, Xunliang Cai, Weinan E, Lei Wu. <br />
under review, arXiv preprint, 1-22. May 2025.</p>
</li>
<li><p>[17] <a href="http://arxiv.org/abs/"><b>A Mechanistic Study of Transformer Training Instability under Mixed Precision</b></a> <br />
Shengtao Guo, <font color=sienna size=+0.5><b><b>Mingze Wang</b></b></font>, Jinbo Wang, Lei Wu. <br />
under review. May 2025.</p>
</li>
<li><p>[16] <a href="http://arxiv.org/abs/2505.24205"><b>On the Expressive Power of Mixture-of-Experts for Structured Complex Tasks</b></a> <br />
<font color=sienna size=+0.5><b><b>Mingze Wang†</b></b></font>, Weinan E. <br />
<tt>2025 Conference on Neural Information Processing Systems</tt> (<font color=purple size=+0.5><b><b>NeurIPS 2025</b></b></font>) (<font color=red size=+0.5><b>Spotlight, top 3.5%</b></font>), 1-18.</p>
</li>
<li><p>[15] <a href="https://openreview.net/forum?id=KfsMlrl81a"><b>On the Learning Dynamics of Two-layer ReLU Networks with Label Noise SGD</b></a> <br />
Tongcheng Zhang, Zhanpeng Zhou, <font color=sienna size=+0.5><b><b>Mingze Wang</b></b></font>, Andi Han, Wei Huang, Taiji Suzuki, Junchi Yan. <br />
<tt>2026 AAAI Conference on Artificial Intelligence</tt> (<font color=purple size=+0.5><b><b>AAAI 2026</b></b></font>) (<font color=red size=+0.5><b>Oral</b></font>).</p>
</li>
<li><p>[14] <a href="https://openreview.net/forum?id=lWGMbJRCtQ"><b>A Single Global Merging Suffices: Recovering Centralized Learning Performance in Decentralized Learning</b></a> <br />
Tongtian Zhu, Tianyu Zhang, <font color=sienna size=+0.5><b><b>Mingze Wang</b></b></font>, Zhanpeng Zhou, Can Wang. <br />
<tt>ICLR 2025 Workshop on Weight Space Learning</tt> (<font color=purple size=+0.5><b><b>ICLR 2025 - WSL</b></b></font>), 1-23.</p>
</li>
<li><p>[13] <a href="http://arxiv.org/abs/2502.19002"><b>The Sharpness Disparity Principle in Transformers for Accelerating Language Model Pre-Training</b></a> <br />
Jinbo Wang*, <font color=sienna size=+0.5><b><b>Mingze Wang*†</b></b></font>, Zhanpeng Zhou*, Junchi Yan, Weinan E, Lei Wu. <br />
<tt>2025 International Conference on Machine Learning</tt> (<font color=purple size=+0.5><b><b>ICML 2025</b></b></font>), 1-23. <br /></p>
</li>
<li><p>[12] <a href="https://arxiv.org/abs/2410.11474"><b>How Transformers Get Rich: Approximation and Dynamics Analysis</b></a> <br />
<font color=sienna size=+0.5><b><b>Mingze Wang†</b></b></font>, Ruoxi Yu, Weinan E, Lei Wu. <br />
<tt>ICML 2025 Workshop on High-dimensional Learning Dynamics</tt> (<font color=purple size=+0.5><b><b>ICML 2025 - HiLD</b></b></font>), 1-47.</p>
</li>
<li><p>[11] <a href="https://arxiv.org/abs/2410.10373"><b>Sharpness-Aware Minimization Efficiently Selects Flatter Minima Late in Training</b></a> <br />
Zhanpeng Zhou*, <font color=sienna size=+0.5><b><b>Mingze Wang*</b></b></font>, Yuchen Mao, Bingrui Li, Junchi Yan. <br />
<tt>2025 International Conference on Learning Representations</tt> (<font color=purple size=+0.5><b><b>ICLR 2025</b></b></font>) (<font color=red size=+0.5><b>Spotlight, top 5.1%</b></font>), 1-31.</p>
</li>
<li><p>[10] <a href="https://arxiv.org/abs/2405.20763"><b>Improving Generalization and Convergence by Enhancing Implicit Regularization</b></a> <br />
<font color=sienna size=+0.5><b><b>Mingze Wang†</b></b></font>, Jinbo Wang, Haotian He, Zilin Wang, Guanhua Huang, Feiyu Xiong, Zhiyu Li, Weinan E, Lei Wu <br />
<tt>2024 Conference on Neural Information Processing Systems</tt> (<font color=purple size=+0.5><b><b>NeurIPS 2024</b></b></font>), 1-44.</p>
</li>
<li><p>[9] <a href="https://arxiv.org/abs/2402.07193"><b>Loss Symmetry and Noise Equilibrium of Stochastic Gradient Descent</b></a> <br />
Liu Ziyin, <font color=sienna size=+0.5><b><b>Mingze Wang</b></b></font>, Hongchao Li, Lei Wu <br />
<tt>2024 Conference on Neural Information Processing Systems</tt> (<font color=purple size=+0.5><b><b>NeurIPS 2024</b></b></font>), 1-26.</p>
</li>
<li><p>[8] <a href="https://arxiv.org/abs/2402.00522"><b>Understanding the Expressive Power and Mechanisms of Transformer for Sequence Modeling</b></a> <br />
<font color=sienna size=+0.5><b><b>Mingze Wang</b></b></font>, Weinan E <br />
<tt>2024 Conference on Neural Information Processing Systems</tt> (<font color=purple size=+0.5><b><b>NeurIPS 2024</b></b></font>), 1-76.</p>
</li>
<li><p>[7] <a href="https://arxiv.org/abs/2406.01179"><b>Are AI-Generated Text Detectors Robust to Adversarial Perturbations?</b></a> <br />
Guanhua Huang, Yuchen Zhang, Zhe Li, Yongjian You, <font color=sienna size=+0.5><b><b>Mingze Wang</b></b></font>, Zhouwang Yang<br />
<tt>2024 Annual Meeting of the Association for Computational Linguistics</tt> (<font color=purple size=+0.5><b><b>ACL 2024</b></b></font>), 1-20.</p>
</li>
<li><p>[6] <a href="https://arxiv.org/abs/2311.14387"><b>Achieving Margin Maximization Exponentially Fast via Progressive Norm Rescaling</b></a> <br />
<font color=sienna size=+0.5><b><b>Mingze Wang†</b></b></font>, Zeping Min, Lei Wu<br />
<tt>2024 International Conference on Machine Learning</tt> (<font color=purple size=+0.5><b><b>ICML 2024</b></b></font>), 1-38.</p>
</li>
<li><p>[5] <a href="https://arxiv.org/abs/2310.00692"><b>A Theoretical Analysis of Noise Geometry in Stochastic Gradient Descent</b></a> <br />
<font color=sienna size=+0.5><b><b>Mingze Wang</b></b></font>, Lei Wu<br />
<tt>NeurIPS 2023 Workshop on Mathematics of Modern Machine Learning</tt> (<font color=purple size=+0.5><b><b>NeurIPS 2023 - M3L</b></b></font>), 1-30.</p>
</li>
<li><p>[4] <a href="https://arxiv.org/abs/2305.12467"><b>Understanding Multi-phase Optimization Dynamics and Rich Nonlinear Behaviors of ReLU Networks</b></a> <br />
<font color=sienna size=+0.5><b><b>Mingze Wang†</b></b></font>, Chao Ma<br />
<tt>2023 Conference on Neural Information Processing Systems</tt> (<font color=purple size=+0.5><b><b>NeurIPS 2023</b></b></font>) (<font color=red size=+0.5><b>Spotlight, top 3.5%</b></font>), 1-94.</p>
</li>
<li><p>[3] <a href="https://arxiv.org/abs/2207.02628"><b>The alignment property of SGD noise and how it helps select flat minima: A stability analysis</b></a> <br />
Lei Wu, <font color=sienna size=+0.5><b><b>Mingze Wang</b></b></font>, Weijie J. Su<br />
<tt>2022 Conference on Neural Information Processing Systems</tt> (<font color=purple size=+0.5><b><b>NeurIPS 2022</b></b></font>), 1-25.</p>
</li>
<li><p>[2] <a href="https://arxiv.org/abs/2206.02139"><b>Early Stage Convergence and Global Convergence of Training Mildly Parameterized Neural Networks</b></a> <br />
<font color=sienna size=+0.5><b><b>Mingze Wang†</b></b></font>, Chao Ma<br />
<tt>2022 Conference on Neural Information Processing Systems</tt> (<font color=purple size=+0.5><b><b>NeurIPS 2022</b></b></font>), 1-73.</p>
</li>
<li><p>[1] <a href="https://arxiv.org/abs/2206.03299"><b>Generalization Error Bounds for Deep Neural Networks Trained by SGD</b></a> <br />
<font color=sienna size=+0.5><b><b>Mingze Wang†</b></b></font>, Chao Ma<br />
arXiv preprint, 1-32, June 2022.</p>
</li>
</ul>
<h2>Co-authors</h2>
<ul>
<li><p><a href="https://web.math.princeton.edu/~weinan/"><b>Weinan E</b>.</a> Peking University; Princeton University; AI for Science Institute.</p>
</li>
</ul>
<ul>
<li><p><a href="https://www.chaom.org/"><b>Chao Ma</b>.</a> Department of Mathematics, Stanford University.</p>
</li>
</ul>
<ul>
<li><p><a href="https://leiwu0.github.io/index.html"><b>Lei Wu</b>.</a> School of Mathematical Sciences, Peking University.</p>
</li>
</ul>
<ul>
<li><p><a href="http://stat.wharton.upenn.edu/~suw/"><b>Weijie J. Su</b>.</a> Department of Statistics and Data Science, University of Pennsylvania.</p>
</li>
</ul>
<ul>
<li><p><a href="https://www.mit.edu/~ziyinl/"><b>Liu Ziyin</b>.</a> Department of Physics, MIT.</p>
</li>
</ul>
</td>
</tr>
</table>
</body>
</html>
